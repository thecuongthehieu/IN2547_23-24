{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Regression on House Pricing Dataset: Variable Selection & Regularization\n",
    "We consider a reduced version of a dataset containing house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015.\n",
    "\n",
    "https://www.kaggle.com/harlfoxem/housesalesprediction\n",
    "\n",
    "For each house we know 18 house features (e.g., number of bedrooms, number of bathrooms, etc.) plus its price, that is what we would like to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert your ID number (\"numero di matricola\") below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put here your ``numero di matricola''\n",
    "numero_di_matricola = 1 # COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all packages needed\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data, remove data samples/points with missing values (NaN) and take a look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.000000e+02</td>\n",
       "      <td>3.000000e+02</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.526086e+09</td>\n",
       "      <td>5.115117e+05</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>2.013333</td>\n",
       "      <td>2055.496667</td>\n",
       "      <td>14193.926667</td>\n",
       "      <td>1.421667</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>3.456667</td>\n",
       "      <td>7.586667</td>\n",
       "      <td>1705.020000</td>\n",
       "      <td>350.476667</td>\n",
       "      <td>1966.690000</td>\n",
       "      <td>93.236667</td>\n",
       "      <td>98074.943333</td>\n",
       "      <td>47.544087</td>\n",
       "      <td>-122.224943</td>\n",
       "      <td>1952.770000</td>\n",
       "      <td>12555.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.887233e+09</td>\n",
       "      <td>3.419579e+05</td>\n",
       "      <td>0.843362</td>\n",
       "      <td>0.729392</td>\n",
       "      <td>885.183995</td>\n",
       "      <td>27184.787381</td>\n",
       "      <td>0.502200</td>\n",
       "      <td>0.114889</td>\n",
       "      <td>0.770441</td>\n",
       "      <td>0.680440</td>\n",
       "      <td>1.089240</td>\n",
       "      <td>743.761357</td>\n",
       "      <td>481.768276</td>\n",
       "      <td>28.290311</td>\n",
       "      <td>422.119640</td>\n",
       "      <td>52.884617</td>\n",
       "      <td>0.143289</td>\n",
       "      <td>0.134342</td>\n",
       "      <td>662.435198</td>\n",
       "      <td>19990.842449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.151031e+07</td>\n",
       "      <td>1.530000e+05</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>1044.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>580.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1900.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98001.000000</td>\n",
       "      <td>47.197600</td>\n",
       "      <td>-122.451000</td>\n",
       "      <td>830.000000</td>\n",
       "      <td>1106.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.944665e+09</td>\n",
       "      <td>3.039250e+05</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1367.500000</td>\n",
       "      <td>5230.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1167.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1949.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98031.000000</td>\n",
       "      <td>47.427150</td>\n",
       "      <td>-122.339000</td>\n",
       "      <td>1450.000000</td>\n",
       "      <td>5075.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.795251e+09</td>\n",
       "      <td>4.300000e+05</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1960.000000</td>\n",
       "      <td>8088.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1545.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1968.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98059.000000</td>\n",
       "      <td>47.555950</td>\n",
       "      <td>-122.255000</td>\n",
       "      <td>1790.000000</td>\n",
       "      <td>7940.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.235338e+09</td>\n",
       "      <td>6.075000e+05</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2480.000000</td>\n",
       "      <td>11216.250000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2161.250000</td>\n",
       "      <td>650.000000</td>\n",
       "      <td>1988.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98117.000000</td>\n",
       "      <td>47.676350</td>\n",
       "      <td>-122.148500</td>\n",
       "      <td>2332.500000</td>\n",
       "      <td>10391.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.822700e+09</td>\n",
       "      <td>2.900000e+06</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>5420.000000</td>\n",
       "      <td>315374.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>5310.000000</td>\n",
       "      <td>2060.000000</td>\n",
       "      <td>2014.000000</td>\n",
       "      <td>2013.000000</td>\n",
       "      <td>98199.000000</td>\n",
       "      <td>47.773100</td>\n",
       "      <td>-121.711000</td>\n",
       "      <td>4760.000000</td>\n",
       "      <td>193842.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id         price    bedrooms   bathrooms  sqft_living  \\\n",
       "count  3.000000e+02  3.000000e+02  300.000000  300.000000   300.000000   \n",
       "mean   4.526086e+09  5.115117e+05    3.333333    2.013333  2055.496667   \n",
       "std    2.887233e+09  3.419579e+05    0.843362    0.729392   885.183995   \n",
       "min    1.151031e+07  1.530000e+05    1.000000    0.750000   700.000000   \n",
       "25%    1.944665e+09  3.039250e+05    3.000000    1.500000  1367.500000   \n",
       "50%    3.795251e+09  4.300000e+05    3.000000    2.000000  1960.000000   \n",
       "75%    7.235338e+09  6.075000e+05    4.000000    2.500000  2480.000000   \n",
       "max    9.822700e+09  2.900000e+06    6.000000    4.750000  5420.000000   \n",
       "\n",
       "            sqft_lot      floors  waterfront        view   condition  \\\n",
       "count     300.000000  300.000000  300.000000  300.000000  300.000000   \n",
       "mean    14193.926667    1.421667    0.013333    0.220000    3.456667   \n",
       "std     27184.787381    0.502200    0.114889    0.770441    0.680440   \n",
       "min      1044.000000    1.000000    0.000000    0.000000    1.000000   \n",
       "25%      5230.000000    1.000000    0.000000    0.000000    3.000000   \n",
       "50%      8088.500000    1.000000    0.000000    0.000000    3.000000   \n",
       "75%     11216.250000    2.000000    0.000000    0.000000    4.000000   \n",
       "max    315374.000000    3.000000    1.000000    4.000000    5.000000   \n",
       "\n",
       "            grade   sqft_above  sqft_basement     yr_built  yr_renovated  \\\n",
       "count  300.000000   300.000000     300.000000   300.000000    300.000000   \n",
       "mean     7.586667  1705.020000     350.476667  1966.690000     93.236667   \n",
       "std      1.089240   743.761357     481.768276    28.290311    422.119640   \n",
       "min      5.000000   580.000000       0.000000  1900.000000      0.000000   \n",
       "25%      7.000000  1167.500000       0.000000  1949.000000      0.000000   \n",
       "50%      7.000000  1545.000000       0.000000  1968.000000      0.000000   \n",
       "75%      8.000000  2161.250000     650.000000  1988.250000      0.000000   \n",
       "max     12.000000  5310.000000    2060.000000  2014.000000   2013.000000   \n",
       "\n",
       "            zipcode         lat        long  sqft_living15     sqft_lot15  \n",
       "count    300.000000  300.000000  300.000000     300.000000     300.000000  \n",
       "mean   98074.943333   47.544087 -122.224943    1952.770000   12555.270000  \n",
       "std       52.884617    0.143289    0.134342     662.435198   19990.842449  \n",
       "min    98001.000000   47.197600 -122.451000     830.000000    1106.000000  \n",
       "25%    98031.000000   47.427150 -122.339000    1450.000000    5075.000000  \n",
       "50%    98059.000000   47.555950 -122.255000    1790.000000    7940.000000  \n",
       "75%    98117.000000   47.676350 -122.148500    2332.500000   10391.750000  \n",
       "max    98199.000000   47.773100 -121.711000    4760.000000  193842.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the data\n",
    "df = pd.read_csv('kc_house_data_reduced.csv', sep = ',')\n",
    "\n",
    "#remove the data samples with missing values (NaN)\n",
    "df = df.dropna() \n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract input and output data. We want to predict the price by using features other than id as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of data: 300\n"
     ]
    }
   ],
   "source": [
    "Data = df.values\n",
    "# m = number of input samples\n",
    "m = Data.shape[0]\n",
    "print(\"Amount of data:\",m)\n",
    "Y = Data[:m,2]\n",
    "X = Data[:m,3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "\n",
    "We split the data into 3 parts: one will be used for training and choosing the parameters, one for choosing among different models (validation data; we will discuss validation in detail in later lectures), and one for testing (used only at the end to estimate the final generalization error of models). The part for training and choosing the parameters will consist of $m_{train}=200$ samples, the one for choosing among different models will consist of $m_{val}=50$ samples, while the other part consists of $m_{test}=m - m_{traing} - m_{val}$ samples ($m$ is the total number of samples in the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of data for training and deciding parameters: 200\n",
      "Amount of data for validation (choosing among different models): 50\n",
      "Amount of data for test: 50\n"
     ]
    }
   ],
   "source": [
    "# Split data into train (2/3 of samples), validation (1/6 of samples), and test data (the rest)\n",
    "m_train = int(2./3. * m)\n",
    "m_val = int((m - m_train) / 2.)\n",
    "m_test = m - m_train - m_val\n",
    "print(\"Amount of data for training and deciding parameters:\",m_train)\n",
    "print(\"Amount of data for validation (choosing among different models):\",m_val)\n",
    "print(\"Amount of data for test:\",m_test)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain_and_val, Xtest, Ytrain_and_val, Ytest = train_test_split(X, Y, test_size=m_test/m, random_state=numero_di_matricola)\n",
    "Xtrain, Xval, Ytrain, Yval = train_test_split(Xtrain_and_val, Ytrain_and_val, test_size=m_val/ (m_train + m_val), random_state=numero_di_matricola) #split the train_and_val data into training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's standardize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre-processing\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(Xtrain)\n",
    "Xtrain_scaled = scaler.transform(Xtrain)#scaled training data\n",
    "Xtrain_and_val_scaled = scaler.transform(Xtrain_and_val) #scaled training and validation data\n",
    "Xval_scaled = scaler.transform(Xval)#scaled validation data\n",
    "Xtest_scaled = scaler.transform(Xtest)#scaled test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least-Squares Solution\n",
    "\n",
    "Now compute the Least-Squares estimate using LinearRegression() in Scikit-learn, and print the corresponding average loss in training and validation data.\n",
    "\n",
    "Since the average loss can be quite high, we also compute the coefficient of determination $R^2$ and look at $1 - R^{2}$ to have an idea of what the average loss amounts to. To compute the coefficient of determination one can use the \"score(...)\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss in training data:38132086868.98514\n",
      "Average loss in validation data:43301817429.73476\n",
      "1 - coefficient of determination on training data:0.2607734891872495\n",
      "1 - coefficient of determination on validation data:0.6558122410766901\n"
     ]
    }
   ],
   "source": [
    "# Least-Squares\n",
    "from sklearn import linear_model \n",
    "#LR the linear regression model\n",
    "LR = linear_model.LinearRegression()\n",
    "\n",
    "#fit the model on training data\n",
    "#COMPLETE\n",
    "LR.fit(Xtrain_scaled, Ytrain)\n",
    "\n",
    "#obtain predictions on training data\n",
    "#COMPLETE\n",
    "Ytrain_predicted = LR.predict(Xtrain_scaled)\n",
    "\n",
    "#obtain predictions on validation data\n",
    "#COMPLETE\n",
    "Yval_predicted = LR.predict(Xval_scaled)\n",
    "\n",
    "#coefficients from the model\n",
    "w_LR = np.hstack((LR.intercept_, LR.coef_))\n",
    "\n",
    "#average error in training data\n",
    "loss_train = np.linalg.norm(Ytrain - Ytrain_predicted)**2 / m_train #COMPLETE\n",
    "\n",
    "\n",
    "#average error in val data\n",
    "loss_val = np.linalg.norm(Yval - Yval_predicted)**2 / m_val #COMPLETE\n",
    "\n",
    "#print average loss in training data and in validation data\n",
    "print(\"Average loss in training data:\"+str(loss_train))\n",
    "print(\"Average loss in validation data:\"+str(loss_val))\n",
    "\n",
    "#print 1 - coefficient of determination in training data and in validation data\n",
    "print(\"1 - coefficient of determination on training data:\"+str(1 - LR.score(Xtrain_scaled, Ytrain)))#COMPLETE\n",
    "print(\"1 - coefficient of determination on validation data:\"+str(1 - LR.score(Xval_scaled, Yval)))#COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso\n",
    "\n",
    "We use the routine *lasso_path* from *sklearn.linear_model* to compute the \"lasso path\" for different values of the regularization parameter $\\lambda$. Computing the \"lasso path\" corresponds to finding the models with lasso for different values of the $\\lambda$ parameter (which define the \"path\"). You should first fix a grid a possible values of lambda (the variable \"lasso_lams\"). For each entry of the vector \"lasso_lams\" we compute the corresponding model (the i-th column of the vector  \"lasso_coefs\" should contain the coefficients of the linear model computed using lasso_lams[i] as regularization parameter).\n",
    "\n",
    "Note that, in general, the grid should be chosen appropriately.\n",
    "\n",
    "Note that the parameter $\\lambda$ is called $\\alpha$ in the Lasso model from sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import lasso_path\n",
    "\n",
    "# select a grid of possible regularization parameters \n",
    "# (be carefull how this is chosen, you may have to refine the choice after having seen the results)\n",
    "\n",
    "lasso_lams = #COMPLETE\n",
    "\n",
    "# Use the function lasso_path to compute the \"lasso path\", passing in input the lambda values\n",
    "# you have specified in lasso_lams\n",
    "lasso_lams, lasso_coefs, _ = #COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the sparsity (i.e., number of non-zero coefficients) in the estimated coefficients as a function of the regularization parameter $\\lambda$: to this purpose, we compute the number of non-zero entries in the estimated coefficient vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l0_coef_norm = np.zeros(len(lasso_lams),)\n",
    "\n",
    "for i in range(len(lasso_lams)):\n",
    "    l0_coef_norm[i] = sum(lasso_coefs[:,i]!=0)\n",
    "\n",
    "\n",
    "plt.figure(6)\n",
    "plt.plot(lasso_lams, l0_coef_norm, marker='o', markersize=5)\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Number of non-zero coefficients')\n",
    "plt.title('Sparsity Degree')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use k-fold Cross-Validation to fix the regularization parameter\n",
    "\n",
    "Now we need to decide which value of $\\lambda$ to use. In order to do this, we are going to use $k$-fold cross-validation.\n",
    "\n",
    "We are going to use the scikit-learn built-in routine *Lasso* (from the *linear_model* package) to compute the lasso  coefficients.\n",
    "\n",
    "We use *KFold* from *sklearn.model_selection* to split the data i.e. XtrainOLS and YtrainOLS) into the desired number of folds.\n",
    "\n",
    "Then we pick $lasso\\_lam\\_opt$ to be the chosen value for the regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "num_folds = 5\n",
    "\n",
    "kf = KFold(n_splits = num_folds)\n",
    "\n",
    "#loss_ridge_kfold will contain the value of the loss\n",
    "loss_lasso_kfold = np.zeros(len(lasso_lams),)\n",
    "\n",
    "for i in range(len(lasso_lams)):\n",
    "    \n",
    "    #define a lasso model   using Lasso() for the i-th value of lam_values; pass as input the argument tol=0.05\n",
    "    lasso_kfold = # COMPLETE\n",
    "    for train_index, validation_index in kf.split(Xtrain_scaled):\n",
    "        Xtrain_kfold, Xval_kfold = # COMPLETE\n",
    "        Ytrain_kfold, Yval_kfold = # COMPLETE\n",
    "    \n",
    "        #learn the model using the training data from the k-fold\n",
    "        #COMPLETE\n",
    "        \n",
    "        #compute the loss using the validation data from the k-fold\n",
    "        loss_lasso_kfold[i] += np.linalg.norm(Yval_kfold - lasso_kfold.predict(Xval_kfold))**2/len(Yval_kfold)\n",
    "    \n",
    "loss_lasso_kfold /= num_folds\n",
    "\n",
    "\n",
    "#choose the regularization parameter that minimizes the loss\n",
    "lasso_lam_opt = lasso_lams[np.argmin(loss_lasso_kfold)]\n",
    "#learn the model with the best choice for the regularization parameter\n",
    "#COMPLETE\n",
    "#COMPLETE\n",
    "print(\"Best value of the regularization parameter:\", lasso_lam_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the Cross-Validation estimates of the loss as a function of the regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(4)\n",
    "plt.xscale('log')\n",
    "plt.plot(lasso_lams, loss_lasso_kfold, color='b', marker='x')\n",
    "plt.scatter(lasso_lams[np.argmin(loss_lasso_kfold)], loss_lasso_kfold[np.argmin(loss_lasso_kfold)], color='b', marker='o', linewidths=5)\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Validation Error')\n",
    "plt.title('Lasso: choice of regularization parameter')\n",
    "plt.show()\n",
    "print(\"Total number of coefficients:\"+str(len(lasso_kfold.coef_)))\n",
    "print(\"Number of non-zero coefficients:\"+str(sum(lasso_kfold.coef_ != 0)))\n",
    "print(\"Best value of regularization parameter:\"+str(lasso_lam_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's estimate the lasso coefficients using all the training data and the optimal regularization parameter (chosen at previous step)\n",
    "\n",
    "Note that we use all the training data to learn the final model for the optimal regularization parameter (once we have chosen the parameter, there is no reason to use less data than what we have). We leave out only the test data so that we can later use it to estimate the generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Lasso  Coefficients with all data (trainval) for the the optimal value lasso_lam_opt of the regularization paramter\n",
    "\n",
    "#define the model\n",
    "#COMPLETE\n",
    "#fit using the training data\n",
    "#COMPLETE\n",
    "\n",
    "#average loss on training data\n",
    "#COMPLETE\n",
    "#average loss on validation data\n",
    "#COMPLETE\n",
    "\n",
    "#print average loss in training data and in validation data\n",
    "print(\"Average loss in training data:\"+str(loss_train_lasso))\n",
    "print(\"Average loss in validation data:\"+str(loss_val_lasso))\n",
    "\n",
    "#now print 1-  the coefficient of determination on training and on validation data to get an idea to what the average\n",
    "#loss corresponds to\n",
    "print(\"1 - coefficient of determination on training data:\"+str(1 - #COMPLETE)\n",
    "print(\"1 - coefficient of determination on validation data:\"+str(1 - #COMPLETE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We now compare the coefficients from the LR model and the lasso model\n",
    "\n",
    "Note that we do not plot the bias (that is not a coefficient of the features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare LR and lasso coefficients\n",
    "ind = np.arange(1,len(LR.coef_)+1)  # the x locations for the groups\n",
    "width = 0.35       # the width of the bars\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(ind, LR.coef_, width, color='r')\n",
    "rects2 = ax.bar(ind + width, lasso_reg.coef_, width, color='y')\n",
    "ax.legend((rects1[0], rects2[0]), ('LR', 'Lasso'))\n",
    "plt.xlabel('Coefficient Idx')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('LR and Lasso Coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "### Use Ridge regression with cross-validation\n",
    "\n",
    "We perform the L2 regularization for different values of the regularization parameter $\\alpha$ (called $\\lambda$ in class), and use the Scikit-learn function to perform cross-validation (CV).\n",
    "\n",
    "In Ridge regression for scikit learn, the objective function is:\n",
    "\n",
    "$$\n",
    "    ||y - Xw||^2_2 + \\alpha * ||w||^2_2\n",
    "$$\n",
    "\n",
    "\n",
    "Note: the CV in Scikit-learn is by default a *stratified* CV, that means that data is split into train-validation while maintaining the proportion of different classes in each fold.\n",
    "\n",
    "In the code below:\n",
    "- we use RidgeCV() to select the best value of $\\alpha$ with a 5-fold CV with L2 penalty;\n",
    "- we use Ridge() to learn the best model for the best $\\alpha$ for ridge regression using the entire training set\n",
    "\n",
    "Note that RidgeCV() picks some default values of $\\alpha$ to try, but we decide to pass the same values used for the Lasso.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's define the values of alpha to use\n",
    "ridge_alphas = #COMPLETE\n",
    "\n",
    "#define the model using RidgeCV passing the vector of alpha values and the cv value (= number of folds)\n",
    "ridge = #COMPLETE\n",
    "\n",
    "#fit the model on training data\n",
    "#COMPLETE\n",
    "\n",
    "# the attribute 'alpha_' contains the best value of alpha as identified by cross-validation;\n",
    "# let's print it\n",
    "\n",
    "print(\"Best value of parameter alpha according to 5-fold Cross-Validation: \"+str(ridge.alpha_))\n",
    "\n",
    "#define the model using the best alpha; note that various solvers are availalbe, choose\n",
    "# an appropriate one\n",
    "ridge_final = #COMPLETE\n",
    "\n",
    "#fit the model using the best C on the entire training set\n",
    "#COMPLETE\n",
    "\n",
    "#average loss on training data\n",
    "loss_train_ridge = #COMPLETE\n",
    "#average loss on validation data\n",
    "loss_val_ridge = #COMPLETE\n",
    "\n",
    "#print average loss in training data and in validation data\n",
    "print(\"Average loss in training data:\"+str(loss_train_ridge))\n",
    "print(\"Average loss in validation data:\"+str(loss_val_ridge))\n",
    "\n",
    "#now print 1-  the coefficient of determination on training and on validation data to get an idea to what the average\n",
    "#loss corresponds to\n",
    "print(\"1 - coefficient of determination on training data:\"+str(1 - #COMPLETE)\n",
    "print(\"1 - coefficient of determination on validation data:\"+str(1 - #COMPLETE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We now compare the coefficient from the LR model, the lasso model, and the ridge model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare LR and lasso coefficients\n",
    "ind = np.arange(1,len(LR.coef_)+1)  # the x locations for the groups\n",
    "width = 0.25       # the width of the bars\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(ind, LR.coef_, width, color='r')\n",
    "rects2 = ax.bar(ind + width, lasso_reg.coef_, width, color='y')\n",
    "rects3 = ax.bar(ind + 2*width, ridge_final.coef_, width, color='b')\n",
    "ax.legend((rects1[0], rects2[0], rects3[0]), ('LR', 'Lasso', 'Ridge'))\n",
    "plt.xlabel('Coefficient Idx')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('LR, Lasso, and Ridge Coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now use all data but the test one to learn the best model (using the chosen model class and parameter, if any), and estimate its generalization error on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute best LR model on training and validation data\n",
    "#COMPLETE\n",
    "\n",
    "#compute and print 1 - coefficient of determination for the best LR model\n",
    "print(\"1 - coefficient of determination of LR on test data :\"+str(1 - #COMPLETE)\n",
    "\n",
    "#compute best lasso_reg model on training and validation data\n",
    "lasso_reg = #COMPLETE\n",
    "#COMPLETE\n",
    "\n",
    "#compute and print 1 - coefficient of determination for the best lasso model\n",
    "print(\"1 - coefficient of determination of lasso on test data :\"+str(1 - #COMPLETE)\n",
    "\n",
    "#compute best ridge model on training and validation data\n",
    "ridge_final = #COMPLETE\n",
    "#COMPLETE\n",
    "\n",
    "#compute and print 1 - coefficient of determination for the best ridge model\n",
    "print(\"1 - coefficient of determination of ridge on test data :\"+str(1 - #COMPLETE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we use other data (from the future!) to see which model really works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "df = pd.read_csv('kc_house_data_future.csv', sep = ',')\n",
    "\n",
    "#remove the data samples with missing values (NaN)\n",
    "df = df.dropna() \n",
    "\n",
    "df.describe()\n",
    "\n",
    "Data = df.values\n",
    "# m = number of input samples\n",
    "m_future = Data.shape[0]\n",
    "print(m_future)\n",
    "Yfuture = #COMPLETE\n",
    "Xfuture = #COMPLETE\n",
    "\n",
    "Xfuture_scaled = #COMPLETE\n",
    "\n",
    "#compute and print 1 - coefficient of determination for the best LR model\n",
    "print(\"1 - coefficient of determination of LR on future data :\"+str(1 - #COMPLETE)\n",
    "\n",
    "#compute and print 1 - coefficient of determination for the best lasso model\n",
    "print(\"1 - coefficient of determination of lasso on future data :\"+str(1 - #COMPLETE)\n",
    "\n",
    "#compute and print 1 - coefficient of determination for the best ridge model\n",
    "print(\"1 - coefficient of determination of ridge on future data :\"+str(1 - #COMPLETE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
