{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix how many hypotheses we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hypotheses = 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix a random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assign a generalization error to each of our hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_errors = np.random.rand(num_hypotheses)+0.2*np.ones(num_hypotheses)\n",
    "#print(\"gen_errors: \", gen_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how they perform on random samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## My Understanding:\n",
    "We have event A = \"The hypothesis $h$ makes error\" (i.e. $h(x) \\neq y,\\ z = (x,y)$)\n",
    "\n",
    "Random Variable $X = l(h, z) =\n",
    "\\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        0 & h(x) = y \\\\\n",
    "        1 & h(x) \\neq y\n",
    "    \\end{array}\n",
    "\\right.$\n",
    "\n",
    "We have $l(h,z) = X \\sim Bernoulli(p)$ (i.e. $Pr[l(h,z) = 1] = p$)\n",
    "\n",
    "Generalization Error: $L_\\mathcal{D}(h) = E[l(h,z)] = E[X] = p$\n",
    "\n",
    "Training Error: $L_S(h) = \\frac{1}{m} * \\sum\\limits_{i=1}^m l(h, z_i) = \\frac{1}{m} * \\sum\\limits_{i=1}^m X_i \\ (= \\frac{1}{m} * Y\\ (Y \\sim Binomial(m, p)))$\n",
    "\n",
    "**Important Intuition**:\n",
    "- In probability theory and statistics, a probability distribution $\\mathcal{D}$ is the mathematical function that gives the probabilities of events\n",
    "- Generalization Error here is $Pr[A] = \\mathcal{D}(A)$ (i.e. probability of the event A)\n",
    "- Generalization Error $L_\\mathcal{D}(h) = p$ implies that probabilty of making error (i.e. $l(h,z) = 1$) is $p$\n",
    "\n",
    "- $L_S(h)$ is **Relative Frequency** of the event $A$ (i.e. $L_S(h) = f_n(A)$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best training error: 0.0\n",
      "Index of ERM hypothesis: 49\n"
     ]
    }
   ],
   "source": [
    "num_samples = 10\n",
    "num_samples_vector = num_samples*np.ones(num_hypotheses) # each hypothesis has num_samples\n",
    "\n",
    "for i in range(len(num_samples_vector)):\n",
    "    num_samples_vector[i] = int(num_samples_vector[i])\n",
    "    \n",
    "# print(\"num_samples_vector: \", num_samples_vector)\n",
    "\n",
    "outcomes = np.zeros(num_hypotheses)\n",
    "\n",
    "ERM_hyp = -1\n",
    "minimum_ER = 1.\n",
    "for i in range(len(num_samples_vector)): # i is index of ERM_hyp\n",
    "    # outcomes[i] is training error of ith hypothesis\n",
    "    outcomes[i] = np.random.binomial(num_samples_vector[i], min(gen_errors[i],1.0))/num_samples\n",
    "    # print(\"outcomes[\" + str(i) + \"]\", outcomes[i])\n",
    "    if (outcomes[i] < minimum_ER):\n",
    "        minimum_ER = outcomes[i]\n",
    "        ERM_hyp = i\n",
    "    \n",
    "\n",
    "#see what is the best training error\n",
    "print(\"Best training error: \"+str(minimum_ER))\n",
    "print(\"Index of ERM hypothesis: \"+str(ERM_hyp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best generalization error: 0.20000027426658157\n"
     ]
    }
   ],
   "source": [
    "print(\"Best generalization error: \"+str(min(gen_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalization error of ERM hypothesis: 0.22662436477311437\n"
     ]
    }
   ],
   "source": [
    "print(\"Generalization error of ERM hypothesis: \"+str(gen_errors[ERM_hyp]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## My Understanding:\n",
    "\n",
    "- When we increase $num\\_hypotheses$, the **randomness** of $outcomes[i]$ increases\n",
    "\n",
    "- When we increase $num\\_samples$, we have $\\displaystyle \\lim_{m\\to +\\infty} L_S(h) = \\lim_{m\\to +\\infty} f_n(A) = p = L_D(h)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
