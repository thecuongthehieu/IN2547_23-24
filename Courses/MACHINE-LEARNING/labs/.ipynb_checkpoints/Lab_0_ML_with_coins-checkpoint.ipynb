{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_vyQgXehjAuU",
        "6WzeqsG9jf2x"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Relative Frequency, Chebyshev's inequality and Chernoff bound"
      ],
      "metadata": {
        "id": "_vyQgXehjAuU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first need to import the libraries that we are going to use in this notebook.\n",
        "\n",
        "1. The *numpy* Python library is used for mathematical functions, such as our random number generators;\n",
        "2. The *matplotlib* Python library is used to plot graphics and visualize data."
      ],
      "metadata": {
        "id": "wZgXkXwMdc6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(1)"
      ],
      "metadata": {
        "id": "akFUx57VnWWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define our Bernoulli variable $X_j$ simulating the flip of the $j$-th coin:\n",
        "\n",
        "$$\n",
        "X_j =\n",
        "\\begin{cases}\n",
        "1 & \\text{if H with probability $p$} \\\\\n",
        "0 & \\text{if T}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "We know that in a Bernoulli random variable we have that:\n",
        "\n",
        "\n",
        "*   $\\mathbb{E} \\left[ X_j \\right] = p$, and\n",
        "*   $\\mathbb{Var} \\left[ X_j \\right] = p (1 - p)$.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TTaiJyOR6UGC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcJQVc4lifia"
      },
      "outputs": [],
      "source": [
        "# -- function that simulates a coin flipping (Bernoulli variable)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Relative Frequency"
      ],
      "metadata": {
        "id": "0hGtmKnyj30K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's try to estimate the probability $p$ by computing the relative frequency simulating the flip of our coin for $n = 10000$ times.\n",
        "\n",
        "We want to compute and plot the estimated probability for each of such flip:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\tilde{p_i} = \\sum_{j=1}^{i} \\frac{X_j}{i}\n",
        "\\end{equation}\n",
        "\n",
        "for $i = 1, ..., n$"
      ],
      "metadata": {
        "id": "kblfGpQG8Dpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -- let's set our parameters"
      ],
      "metadata": {
        "id": "R-j3slZPfqYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- function that estimates our probability at each flip, by counting the heads flipped at such flip"
      ],
      "metadata": {
        "id": "EpOuCj9X8D_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's plot the estimated probability for each flip"
      ],
      "metadata": {
        "id": "8kHQeRI1_MD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -- plot the estimated probability\n",
        "plt.plot(estimated_p, color = 'b', label='Estimated probability')\n",
        "\n",
        "# plot true value\n",
        "plt.axhline(y=p, color='r', linestyle='--', label='True probability')\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "plt.xlabel('Number of launches')\n",
        "plt.ylabel('Estimated probability')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "p2jEGn9Iqre2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- let' see what is actually the true probability"
      ],
      "metadata": {
        "id": "eXMCzeNWMkZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we want to compute the mean and the variance of our random variable $\\tilde{p_n}$, that we recall is defined as:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\tilde{p_n} = \\sum_{j = 1}^{n} \\frac{X_j}{n}\n",
        "\\end{equation}\n",
        "\n",
        "So, we have that the expected value at the end of our $n$ flips is:\n",
        "\n",
        "\\begin{align}\n",
        "  \\mathbb{E}\\left[\\tilde{p_n}\\right] &= \\mathbb{E} \\left[ \\sum_{j = 1}^{n} \\frac{X_j}{n} \\right] = \\frac{1}{n} \\ \\mathbb{E} \\left[ \\sum_{j = 1}^{n} X_j \\right] = \\\\\n",
        "  & = \\frac{1}{n} \\ \\sum_{j = 1}^{n} \\ \\mathbb{E} \\left[ X_j \\right] = \\frac{1}{n} \\ \\sum_{j = 1}^{n} \\ p = p\n",
        "\\end{align}\n",
        "\n",
        "and the variance is computed as:\n",
        "\n",
        "\\begin{align}\n",
        "  \\mathbb{Var} \\left[\\tilde{p_n}\\right] &= \\mathbb{Var} \\left[ \\sum_{j = 1}^{n} \\frac{X_j}{n} \\right] = \\frac{1}{n^2} \\ \\mathbb{Var} \\left[ \\sum_{j = 1}^{n} X_j \\right] = \\\\\n",
        "  & = \\frac{1}{n^2} \\ \\sum_{j = 1}^{n} \\ \\mathbb{Var} \\left[ X_j \\right] = \\frac{1}{n^2} \\ n \\ \\mathbb{Var} \\left[ X_j \\right] = \\frac{p (1 - p)}{n}\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "ww511XaW_7Tm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's analyze, in our experiments, the computations of the empirical mean:"
      ],
      "metadata": {
        "id": "TUdtGdNBjlKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_mean =\n",
        "print('Mean of X = ', X_mean)\n",
        "print('p = ', p)"
      ],
      "metadata": {
        "id": "uwco-RcKd1dF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chebyshev's inequality\n",
        "\n",
        "As you have seen during the lectures, given a random variable $X$ with $\\mathbb{E}\\left[X\\right] = \\mu$ and $\\mathbb{Var}\\left[X\\right] = \\sigma^2$, and a fixed constant $\\varepsilon > 0 $, the Chebyshev's inequality states:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\mathbb{P}\\left[ \\left| X - \\mu \\right| > \\varepsilon \\right] \\leq \\frac{\\sigma^2}{\\varepsilon^2}\n",
        "\\end{equation}\n",
        "\n",
        "In our example, we want to plot the Chebyshev bounds given by the above inequality for the estimated probability in each of our coin flips.\n",
        "\n",
        "So, if we let:\n",
        "\n",
        "  * $X = \\tilde{p_i}$\n",
        "  * $\\mu = \\mathbb{E}\\left[ \\tilde{p_i} \\right] = p$\n",
        "  * $\\sigma_i^2 = \\mathbb{Var}\\left[ \\tilde{p_i} \\right] = \\frac{p (1 - p)}{i}$\n",
        "  * $\\varepsilon = \\varepsilon\\mu$\n",
        "\n",
        "we obtain:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\mathbb{P}\\left[ | \\tilde{p_i} - p | > \\varepsilon p \\right] \\leq \\frac{p (1 - p)}{i \\varepsilon^2 p^2}\n",
        "\\end{equation}\n"
      ],
      "metadata": {
        "id": "k9dj4qYej7S5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Let's compute and plot the probability bounds of being far from the true mean, for each of our $\\tilde{p_i}$ random variables, $i = 1, ..., n$.\n",
        "\n",
        "Let's fix the right-hand-side of the above inequality, by letting $\\delta = \\frac{p (1 - p)}{i \\varepsilon^2 p^2} = 0.05$, that corresponds to the probability of obtaining \"bad\" estimates. We need to compute the distance from the mean (i.e., $\\varepsilon p$) for each flip $i$, that is:\n",
        "\n",
        "\\begin{equation}\n",
        "  É› p = \\sqrt{\\frac{p(1 - p)}{\\delta i}}\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "XsaSI1cbqfhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fix delta\n",
        "\n",
        "\n",
        "plt.plot(estimated_p, color = 'b', label='Estimated probability')\n",
        "# -- plot true value\n",
        "plt.axhline(y = p, color = 'r', linestyle = '--', label='True probability')\n",
        "\n",
        "# -- Chebyshev's bounds\n",
        "# -- compute the Chebyshev values for each flip\n",
        "\n",
        "plt.plot(, color = 'g', linestyle='--')\n",
        "plt.plot(, color = 'g', linestyle='--', label=f'Chebyshev bounds with delta={delta}')\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "\n",
        "plt.xlabel('Number of launches')\n",
        "plt.ylabel('Estimated probability')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "gANpJklOn7zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try with different values of $\\delta$, that reflect the probability of optaining good approximation."
      ],
      "metadata": {
        "id": "eH60mONbtndL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chernoff's bound\n",
        "\n",
        "Now, we are going to analyze a more precise tool for the approximation guarantees: the Chernoff's bound (you will use it in the next lectures).\n"
      ],
      "metadata": {
        "id": "C3D65mKRj-uc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Chernoff's bound:**\n",
        "Given a random variable $X = \\sum_{i=1}^n X_i$, written as a sum of Bernoulli random variables $X_i$ with $\\mu = \\mathbb{E} \\left[X \\right]$, and a small constant $\\varepsilon \\in (0, 1)$, we have that:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\mathbb{P}\\left[ \\left| X - \\mu \\right| > \\varepsilon \\mu \\right] \\leq 2 e^{\\frac{\\varepsilon^2 \\mu}{3}}\n",
        "\\end{equation}\n",
        "\n",
        "In our example, we need to rearrange the variables. In particular, we want (as done for the Chebyshev's bounds) to compute the following approximation:\n",
        "\n",
        "\\begin{align}\n",
        "  & \\mathbb{P}\\left[ | \\tilde{p_i} - p | > \\varepsilon p \\right] \\\\\n",
        "  & = \\mathbb{P}\\left[ \\big | \\sum_{j=1}^i \\frac{X_j}{i} - p \\ \\big | > \\varepsilon p \\right] \\\\\n",
        "  & = \\mathbb{P}\\left[ \\big | \\sum_{j=1}^i X_j - p \\ i \\ \\big | > \\varepsilon p \\ i \\right] \\\\\n",
        "  & \\leq 2 e^{\\varepsilon^2 p\\ i / 3}\n",
        "\\end{align}\n",
        "\n",
        "Also in this case, let's fix $\\delta = 2 e^{\\varepsilon^2 p\\ i / 3} = 0.05$, the probability of optaining \"bad\" estimates.\n",
        "\n"
      ],
      "metadata": {
        "id": "EF1of5cJz4KI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can similarly plot the Chernoff's bounds by computing the distances from the true mean (i.e., $É› p$ as defined in the above) for each coin flip."
      ],
      "metadata": {
        "id": "ot89px0g4CEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fix delta\n",
        "\n",
        "plt.plot(estimated_p, color = 'b', label='Estimated probability')\n",
        "# plot true value\n",
        "plt.axhline(y = p, color = 'r', linestyle = '--', label='True probability')\n",
        "\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "# -- Chernoff's bounds\n",
        "# -- compute the chernoff values for each flip\n",
        "\n",
        "plt.plot(, color = 'k', linestyle='--')\n",
        "plt.plot(, color = 'k', linestyle='--', label=f'Chernoff bounds with delta={delta}')\n",
        "\n",
        "\n",
        "plt.xlabel('Number of launches')\n",
        "plt.ylabel('Estimated probability')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "dO0sOH844_N9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare Chebyshev's and Chernoff's bounds"
      ],
      "metadata": {
        "id": "TWVErdY-467T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fix delta\n",
        "\n",
        "plt.plot(estimated_p, color = 'b', label='Estimated probability')\n",
        "# plot true value\n",
        "plt.axhline(y = p, color = 'r', linestyle = '--', label='True probability')\n",
        "\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "# -- Chebyshev's bounds\n",
        "chebyshev_values =\n",
        "plt.plot(, color = 'g', linestyle='--')\n",
        "plt.plot(, color = 'g', linestyle='--', label=f'Chebyshev bounds with delta={delta}')\n",
        "\n",
        "# -- Chernoff's bounds\n",
        "chernoff_values =\n",
        "plt.plot(, color = 'k', linestyle='--')\n",
        "plt.plot(, color = 'k', linestyle='--', label=f'Chernoff bounds with delta={delta}')\n",
        "\n",
        "\n",
        "plt.xlabel('Number of launches')\n",
        "plt.ylabel('Estimated probability')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "JCr06vUGgJYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Law of Large Numbers\n",
        "\n",
        "From the plots above, we can also see the validiy of the Law of Large Numbers, that states:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\lim_{n\\to\\infty} \\mathbb{P} \\left[  \\big| \\frac{1}{n} \\sum_{i = 1}^n X_i - \\mu \\big| > É› \\right] = 0.\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "BbC2Hjbj7nRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Coin flipping: a Machine Learning case study\n",
        "\n",
        "We will now exploit the concepts we have introduced to build a simplified Machine Learning case study starting from simple coin flippings. We will consider a coin $j$ as an hypothesis $h$ belonging to our hypothesis set $H$. Each coin $j$ will have an associated probability error $p^{(j)}$, which is not known in advance.\n",
        "\n",
        "Our aim is to find the coin $j$ that has the smallest error probability $p^{(j)}$ (over all $j$) by empirically counting the number of heads in a series of $n$ flips. In other words, we are trying to find the coin that **minimizes the number of heads** in a series of $n$ flips. Let $\\tilde{p_n}^{(j)}$ be the estimated head probability of the $j$-th coin after $n$ flips. We define our training error by means of the $0-1$ loss function as:\n",
        "\n",
        "\\begin{equation}\n",
        "L(j) = \\tilde{p_n}^{(j)}\n",
        "\\end{equation}\n",
        "\n",
        "which penalizes coins that are more likely to flip head, that is indeed what we are looking for. We will select our best coin (hypothesis) from $H$ by means of the Empirical Risk Minimization (ERM) procedure:\n",
        "\n",
        "\\begin{equation}\n",
        "ERM_H \\in argmin_{\\ j \\ \\in \\ H} \\ L(j)\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "6WzeqsG9jf2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Empirical Risk Minimization"
      ],
      "metadata": {
        "id": "td-KXNi6kY_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us first define the ERM procedure, given an initial amount of coins (*n_coins*) and the total number of flips $n$ (*n_flips*). We want to obtain in output the coin that minimizes our loss function, which in our case corresponds to the coin with the lowest rate of heads. We will include an additional parameter (*top_k*) to allow the ERM to output the top $k$ best hypotheses found in our hypothesis set $H$. Note that this is not usually feasible in real Machine Learning problems."
      ],
      "metadata": {
        "id": "7E6NN2OKjrK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -- function that simulates the Empirical Minimization Risk procedure over a set of coins"
      ],
      "metadata": {
        "id": "51GyW_qtjvz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A first experiment"
      ],
      "metadata": {
        "id": "fYpq4mFHkeYt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are ready to make our first experiment using the ERM procedure defined above. We will run the ERM procedure several times by varying the number of coins we are considering (i.e., we are modifying the cardinality of $H$), with a fixed amount of flips. Note that the number of **flips** $n$ (*n_flips* in the code) can be seen as the total amount of **training samples** we are using to estimate the performance of our coins. We will then perform additional flips to access the generalization error of our coin and see how it behaves."
      ],
      "metadata": {
        "id": "SZoy3BSJjytn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -- let's train our models to see which one has the best training loss\n",
        "\n",
        "# -- now we need to test the best coin to see if it is actually good\n"
      ],
      "metadata": {
        "id": "UiqXgQZdj2zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because of the small amount of flips we have made, the ERM procedure selects a coin that actually performs poorly in the test flips. This is due to the fact that we have selected our coin according to a small amount of training samples (i.e. flips), which does not allow to collect a meaningful measure of our coin's performance. We will now repeat the experiment with more data (more flips) and plot the training loss of the best coin obtained. We iterate the process for different amounts of coins and see what happens.\n"
      ],
      "metadata": {
        "id": "-eVFYjsKj8Wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -- let's perform some experiments with more flips\n",
        "# -- we will also vary the number of total coins considered to see what happens"
      ],
      "metadata": {
        "id": "i5CvrbuvkBFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- plot the losses obtained\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Number of coins')\n",
        "plt.ylabel('Training loss')"
      ],
      "metadata": {
        "id": "se5mRiNtFqp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the plot above we can see that, when we increase the amount of coins (that is, a larger hypothesis set $H$) we have a higher chance to find better coins that perform well on the training set. Recall, however, that we are interested in obtaining a coin that is sufficiently good also on the test set. In the next section, we will try to see if the best coin we obtain with the ERM procedure is actually good also on the test flips."
      ],
      "metadata": {
        "id": "3kvqk-5MLXeM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generalization error"
      ],
      "metadata": {
        "id": "mRcbg7h4kn0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We finally try to collect an estimate on the generalization error of our best coin by executing additional flips. There are two possible outcomes:\n",
        "\n",
        "1. The coin returned by the ERM obtains the lowest generalization error among all the coins we have considered, and we have succeded;\n",
        "2. There is another coin (not usually considered by the ERM) that has a lower generalization error than the best coin: we are in an overfitting scenario. Indeed, the training set of the coin returned by the ERM may be not representative of the true distribution we are considering."
      ],
      "metadata": {
        "id": "rXCwD8LJkD9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -- it is now time to test if the ERM procedure is always providing the best model for us"
      ],
      "metadata": {
        "id": "Uc6rAVhlkI11"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}